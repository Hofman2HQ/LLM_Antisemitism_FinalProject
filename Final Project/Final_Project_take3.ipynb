{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0K-Cqvzl0TL",
        "outputId": "9f4bbc56-c7a8-40fa-f892-42521a4fc528"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.15.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.7.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0MaQjAblR37",
        "outputId": "e6fbde6d-32bd-46be-97fe-8e7a3739d706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# First Block of Code\n",
        "\n",
        "# 1) Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# 2) Import several CSV files\n",
        "file1 = 'consolidated_hand_labels_fixed.csv'\n",
        "file2 = 'tropes_labeled_training.csv'\n",
        "file3 = 'labeled_data.csv'\n",
        "\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "df3 = pd.read_csv(file3)\n",
        "\n",
        "# Adjusting the dataframes to have consistent columns: 'text' and 'label'\n",
        "\n",
        "# Adjust df1: Already has 'text' and 'label'\n",
        "df1 = df1[['text', 'label']]\n",
        "\n",
        "# Adjust df2: Use 'post_text_clean' as the text and derive a binary label\n",
        "df2['label'] = np.where(df2['jewish_mentions_count'] > 0, 1, 0)  # Simplified assumption for labeling\n",
        "df2 = df2[['post_text_clean', 'label']]\n",
        "df2.rename(columns={'post_text_clean': 'text'}, inplace=True)\n",
        "\n",
        "# Adjust df3: Use 'tweet' as the text and map 'class' to a binary label\n",
        "df3['label'] = np.where(df3['class'] == 0, 1, 0)  # Assuming class 0 represents antisemitic speech\n",
        "df3 = df3[['tweet', 'label']]\n",
        "df3.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "\n",
        "# 3) Combine the different CSV files into one to train the model\n",
        "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "# Data Augmentation\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_texts = []\n",
        "augmented_labels = []\n",
        "\n",
        "for text, label in zip(df_combined['text'], df_combined['label']):\n",
        "    augmented_texts.append(aug.augment(text))\n",
        "    augmented_labels.append(label)\n",
        "\n",
        "# Create a new DataFrame with the augmented data\n",
        "df_augmented = pd.DataFrame({'text': augmented_texts, 'label': augmented_labels})\n",
        "\n",
        "# Combine the original and augmented data\n",
        "df_combined_augmented = pd.concat([df_combined, df_augmented], ignore_index=True)\n",
        "\n",
        "# Tokenize the augmented data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_combined_augmented['text'])\n",
        "sequences_augmented = tokenizer.texts_to_sequences(df_combined_augmented['text'])\n",
        "padded_sequences_augmented = pad_sequences(sequences_augmented, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "# Split the augmented data into training and testing sets\n",
        "X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(padded_sequences_augmented, df_combined_augmented['label'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Create the Model, currently we're using NN\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 2) Compile the Model with Hyperparameter Tuning\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3) Train the Model with Augmented Data\n",
        "history_augmented = model.fit(X_train_augmented, y_train_augmented, epochs=20, validation_data=(X_test_augmented, y_test_augmented), batch_size=64)\n",
        "\n",
        "# 4) Test the Model and Print the Evaluation\n",
        "y_pred_augmented = (model.predict(X_test_augmented) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_augmented, y_pred_augmented))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_augmented, y_pred_augmented))\n",
        "print(\"Accuracy Score:\\n\", accuracy_score(y_test_augmented, y_pred_augmented))\n",
        "\n",
        "# Plotting the Training and Validation Accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_augmented.history['accuracy'], label='train accuracy')\n",
        "plt.plot(history_augmented.history['val_accuracy'], label='validation accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting the Training and Validation Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_augmented.history['loss'], label='train loss')\n",
        "plt.plot(history_augmented.history['val_loss'], label='validation loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9gejongmegf",
        "outputId": "3e2d1be4-1387-4429-fa61-851533254848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m344/766\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 482ms/step - accuracy: 0.8611 - loss: 0.4022"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Allow for User Input in Either Free Text or URL of a Social Media Post\n",
        "def get_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return soup.get_text()\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching the URL content:\", e)\n",
        "        return None\n",
        "\n",
        "user_input = input(\"Enter the text or URL: \")\n",
        "if user_input.startswith(\"http\"):\n",
        "    user_text = get_text_from_url(user_input)\n",
        "else:\n",
        "    user_text = user_input\n",
        "\n",
        "if user_text:\n",
        "    # 2) Return What is the Classification of the Text\n",
        "    seq = tokenizer.texts_to_sequences([user_text])\n",
        "    padded = pad_sequences(seq, maxlen=100, padding='post', truncating='post')\n",
        "    prediction = model.predict(padded)[0][0]\n",
        "    print(f\"The probability of this text being antisemitic is {prediction:.2f}\")\n",
        "else:\n",
        "    print(\"Could not process the input.\")"
      ],
      "metadata": {
        "id": "HcVJNdTKm-10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}